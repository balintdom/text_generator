{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def cleaning(text):\n",
    "    \"\"\"\n",
    "        Filter the text from punctuations and stopwords. Lemmatize the words in the text.\n",
    "    \"\"\"\n",
    "        \n",
    "    # clear the text from punctuation\n",
    "    punct = string.punctuation.replace('\\'', '')\n",
    "    text = [word.translate(\"\".maketrans(punct, \" \"*len(punct))) for word in text.split()]\n",
    "\n",
    "    # lemmatize text\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    text = [stemmer.lemmatize(word).lower() for word in text]\n",
    "    \n",
    "    sw = stopwords.words('english')\n",
    "    text = [word for word in text if word not in sw]\n",
    "    \n",
    "    # remove unecessary spaces\n",
    "    text = re.sub('\\s+', ' ', ' '.join(text).strip())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#load and clean the moliere complete dataset\n",
    "with open('./data/moliere_complete.txt', 'r') as f:\n",
    "    moliere = ''.join(list(f))\n",
    "    clean_moliere = cleaning(moliere)\n",
    "dataset_moliere = [clean_moliere[i*100:(i+1)*100] for i in range(int(len(clean_moliere)/100))][:3000]\n",
    "\n",
    "#load and clean the shakespeare complete dataset\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', \n",
    "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "shakespeare = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "clean_shakespeare= cleaning(shakespeare)\n",
    "dataset_shakespeare = [clean_shakespeare[i*100:(i+1)*100] for i in range(int(len(clean_shakespeare)/100))][:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# load and clear the generated datasets\n",
    "with open('./result/lstm_moliere.pkl', 'rb') as f:\n",
    "    lstm_moliere = [cleaning(i)[:100] for i in pkl.load(f)]\n",
    "    lstm_moliere = [i for i in lstm_moliere if len(i)==100]\n",
    "    \n",
    "with open('./result/lstm_shakespeare.pkl', 'rb') as f:\n",
    "    lstm_shakespeare = [cleaning(i)[:100] for i in pkl.load(f)]\n",
    "    lstm_shakespeare = [i for i in lstm_shakespeare if len(i)==100]\n",
    "\n",
    "with open('./result/transformer_moliere.pkl', 'rb') as f:\n",
    "    transformer_moliere = [cleaning(i)[:100] for i in pkl.load(f)]\n",
    "    transformer_moliere = [i for i in lstm_moliere if len(i)==100]\n",
    "    \n",
    "with open('./result/transformer_shakespeare.pkl', 'rb') as f:\n",
    "    transformer_shakespeare = [cleaning(i)[:100] for i in pkl.load(f)]\n",
    "    transformer_shakespeare = [i for i in lstm_shakespeare if len(i)==100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_input_output(moliere, shakespeare):\n",
    "    dataset = moliere+shakespeare\n",
    "    # create labels \n",
    "    Y = np.zeros([len(dataset), 2])\n",
    "\n",
    "    Y[:len(moliere), 0] = 1\n",
    "    Y[len(shakespeare):, 1] = 1\n",
    "\n",
    "    # tokenize the text\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "    # create the data matrix, truncate it to 100 length\n",
    "    x = tokenizer.texts_to_sequences(dataset)\n",
    "    X = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=100)\n",
    "    print(len(tokenizer.word_index))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# create the input matrices from the original datasets\n",
    "X, Y = create_input_output(dataset_moliere, dataset_shakespeare)\n",
    "\n",
    "# create the input matrices from the generated datasets\n",
    "# for transformer experiments change to transformer_...\n",
    "X_aug, Y_aug = create_input_output(lstm_moliere, lstm_shakespeare)\n",
    "\n",
    "# train, validation and test splits\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.1)\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.1*(1/0.9))\n",
    "\n",
    "# concatenate the generated datasets to the train\n",
    "X_train = np.concatenate([X_train, X_aug])\n",
    "Y_train = np.concatenate([Y_train, Y_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# size of the english character vocabulary\n",
    "vocab_size = 38\n",
    "\n",
    "# define early stopping\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, restore_best_weights=True)\n",
    "\n",
    "# create a simple LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=vocab_size+1,\n",
    "    output_dim=64,\n",
    "    trainable=True))\n",
    "model.add(LSTM(units=128, name='lstm_layer_1'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=256, activation='relu', name='dense_layer_1'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=2, name='output_layer', activation='softmax'))\n",
    "\n",
    "#define optimizer function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', \n",
    "      optimizer =  optimizer,\n",
    "      metrics   =  ['acc'])\n",
    "\n",
    "#print the average accuracy of 10 run\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(x=X_train, y = Y_train, validation_data=(X_valid, Y_valid), epochs=20, callbacks = [callback])\n",
    "    result = model.evaluate(X_test, Y_test)\n",
    "    results.append(result[1])\n",
    "\n",
    "print(sum(results)/len(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
